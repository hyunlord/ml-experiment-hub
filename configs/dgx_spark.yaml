# =============================================================================
# DGX Spark (GB10) — Production Training Preset
# =============================================================================
# Pre-tuned for NVIDIA DGX Spark with GB10 Grace-Blackwell (128 GB unified).
# Usage:
#   1. From the Hub UI: Create Experiment → select "DGX Spark" preset
#   2. CLI: python train.py --config configs/dgx_spark.yaml
# =============================================================================

model:
  backbone: "google/siglip2-base-patch16-224"
  freeze_backbone: true
  bit_list: [8, 16, 32, 64, 128]
  hidden_dim: 512
  hash_activation: "tanh"

data:
  # Primary dataset — COCO 2017 with Korean captions
  jsonl_path: "/ml-data/coco_ko/coco_ko.jsonl"
  data_root: "/ml-data/coco"
  image_size: 224
  max_text_len: 77
  num_workers: 12
  pin_memory: true
  # Optional extra datasets (keys from DATASET_REGISTRY)
  # extra_datasets:
  #   - "aihub"
  #   - "cc3m_ko"

training:
  batch_size: "auto"            # GPU auto-configure (see /api/system/gpu-info)
  max_epochs: 30
  learning_rate: 0.0003
  weight_decay: 0.01
  warmup_steps: 500
  optimizer: "adamw"
  scheduler: "cosine"
  gradient_clip_val: 1.0
  val_check_interval: 0.25      # Validate 4x per epoch
  precision: "bf16-mixed"       # BF16 on Grace-Blackwell

checkpoint:
  save_top_k: 3
  monitor: "val/map_64"
  mode: "max"
  every_n_epochs: 1

logging:
  log_every_n_steps: 50
  enable_progress_bar: true

monitor:
  enabled: true
  # server_url and run_id are injected by the Hub at launch time
